{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet Distributed Training (MNIST Sample) with Azure ML service\n",
    "\n",
    "See \"[MXNet Distributed Training Example for Azure ML service](https://tsmatz.wordpress.com/2019/01/17/azure-machine-learning-service-custom-amlcompute-and-runconfig-for-mxnet-distributed-training/)\" for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "\n",
    "Before starting, you must create Azure Machine Learning service workspace and prepare config settings.\n",
    "\n",
    "See [here](https://github.com/tsmatz/azure-ml-tensorflow-complete-sample/blob/master/Readme.md) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save Script for MXNet Distributed Training (mnist_distributed.py)\n",
    "\n",
    "Note : Use commented lines for your debugging in local (with 1 CPU device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/mnist_distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/mnist_distributed.py\n",
    "import os, random\n",
    "import mxnet as mx\n",
    "from mxnet import kv, gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "store = kv.create('dist')\n",
    "\n",
    "gpus_per_machine = 1\n",
    "batch_size_per_gpu = 64\n",
    "batch_size = batch_size_per_gpu * gpus_per_machine\n",
    "num_epochs = 5\n",
    "\n",
    "ctx = [mx.gpu(i) for i in range(gpus_per_machine)]\n",
    "# ctx = mx.cpu(0)\n",
    "\n",
    "class SplitSampler(gluon.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    length: Number of examples in the dataset\n",
    "    num_parts: Partition the data into multiple parts\n",
    "    part_index: The index of the part to read from\n",
    "    \"\"\"\n",
    "    def __init__(self, length, num_parts, part_index):\n",
    "        self.part_len = length // num_parts\n",
    "        self.start = self.part_len * part_index\n",
    "        self.end = self.start + self.part_len\n",
    "    def __iter__(self):\n",
    "        indices = list(range(self.start, self.end))\n",
    "        random.shuffle(indices)\n",
    "        return iter(indices)\n",
    "    def __len__(self):\n",
    "        return self.part_len\n",
    "\n",
    "mx.random.seed(42)\n",
    "def data_xform(data):\n",
    "    \"\"\"Move channel axis to the beginning, cast to float32, and normalize to [0, 1]\"\"\"\n",
    "    return nd.moveaxis(data, 2, 0).astype('float32') / 255\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=True).transform_first(data_xform),\n",
    "    batch_size=batch_size,\n",
    "    sampler=SplitSampler(50000, store.num_workers, store.rank))\n",
    "test_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=False).transform_first(data_xform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "# train_data = gluon.data.DataLoader(\n",
    "#     gluon.data.vision.MNIST(train=True, root='./data').transform_first(data_xform),\n",
    "#     batch_size=batch_size)\n",
    "# test_data = gluon.data.DataLoader(\n",
    "#     gluon.data.vision.MNIST(train=False, root='./data').transform_first(data_xform),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False)\n",
    "\n",
    "net = nn.HybridSequential(prefix='MLP_')\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(128, activation='relu'),\n",
    "        nn.Dense(64, activation='relu'),\n",
    "        nn.Dense(10, activation=None)\n",
    "    )\n",
    "\n",
    "net.hybridize()\n",
    "\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    params=net.collect_params(),\n",
    "    optimizer='sgd',\n",
    "    optimizer_params={'learning_rate': 0.07},\n",
    "    kvstore=store)\n",
    "# trainer = gluon.Trainer(\n",
    "#     params=net.collect_params(),\n",
    "#     optimizer='sgd',\n",
    "#     optimizer_params={'learning_rate': 0.07},\n",
    "# )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\" Train ! \"\"\"\n",
    "    for batch in train_data:\n",
    "        inputs = gluon.utils.split_and_load(batch[0], ctx)\n",
    "        labels = gluon.utils.split_and_load(batch[1], ctx)\n",
    "        # inputs = batch[0].as_in_context(ctx)\n",
    "        # labels = batch[1].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            loss = [loss_function(net(X), Y) for X, Y in zip(inputs, labels)]\n",
    "            # loss = loss_function(net(inputs), labels)\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "        # loss.backward()\n",
    "        trainer.step(batch_size=batch[0].shape[0])\n",
    "    \"\"\" Evaluate and Output ! \"\"\"\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, (test_input, test_label) in enumerate(test_data):\n",
    "        test_input = test_input.as_in_context(ctx[0])\n",
    "        test_label = test_label.as_in_context(ctx[0])\n",
    "        # test_input = test_input.as_in_context(ctx)\n",
    "        # test_label = test_label.as_in_context(ctx)\n",
    "        test_output = net(test_input)\n",
    "        test_pred = nd.argmax(test_output, axis=1)\n",
    "        metric.update(preds=test_pred, labels=test_label)\n",
    "    print('Epoch %d: Accuracy %f' % (epoch, metric.get()[1]))\n",
    "\n",
    "\"\"\" Save Model (both architecture and parameters) \"\"\"\n",
    "if store.rank == 0:\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    net.export('./outputs/test', epoch=1)\n",
    "# os.makedirs('./outputs', exist_ok=True)\n",
    "# net.export('./outputs/test', epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Script to Start Each Roles (start_mx_role.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/start_mx_role.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/start_mx_role.py\n",
    "import argparse\n",
    "import os\n",
    "from mpi4py import MPI\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help='Specifies how many worker roles')\n",
    "parser.add_argument(\n",
    "    '--num_servers',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help='Specifies how many server roles')\n",
    "parser.add_argument(\n",
    "    '--scheduler_host',\n",
    "    type=str,\n",
    "    default='10.0.0.4',\n",
    "    help='Specifies the IP of the scheduler')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "#\n",
    "# See https://mxnet.incubator.apache.org/faq/distributed_training.html\n",
    "#\n",
    "\n",
    "mpi_comm = MPI.COMM_WORLD\n",
    "mpi_rank = mpi_comm.Get_rank()\n",
    "if mpi_rank == 0 :\n",
    "    # Rank 0 is scheduler\n",
    "    os.environ['DMLC_ROLE'] = 'scheduler'\n",
    "elif mpi_rank <= FLAGS.num_servers :\n",
    "    # Rank 1, ..., FLAGS.num_servers is server\n",
    "    os.environ['DMLC_ROLE'] = 'server'\n",
    "else :\n",
    "    # Others are all workers (The count of workers must equal to FLAGS.num_workers.)\n",
    "    os.environ['DMLC_ROLE'] = 'worker'\n",
    "os.environ['DMLC_PS_ROOT_URI'] = FLAGS.scheduler_host\n",
    "os.environ['DMLC_PS_ROOT_PORT'] = '9092'\n",
    "os.environ['DMLC_NUM_WORKER'] = str(FLAGS.num_workers)\n",
    "os.environ['DMLC_NUM_SERVER'] = str(FLAGS.num_servers)\n",
    "\n",
    "#\n",
    "# Run previous script !\n",
    "#\n",
    "import mnist_distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read Workspace Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/tsmatsuz/mxnet/aml_config/config.json\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Cluster (Nodes)\n",
    "\n",
    "Total 4 nodes : scheduler, parameter server, worker0, worker1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new.\n",
      "Creating\n",
      "Succeeded..................\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    " \n",
    "# Create AML compute (or Get existing one)\n",
    "# (Total 4 : scheduler, server, worker1, worker2)\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='cluster01')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_NC6',\n",
    "        min_nodes=4,\n",
    "        max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, 'cluster01', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig, Experiment, Run\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    " \n",
    "conda_dep = CondaDependencies.create()\n",
    "conda_dep.add_pip_package('mxnet-cu90');\n",
    "conda_dep.add_pip_package('mpi4py');\n",
    "run_config = RunConfiguration(\n",
    "    framework='python',\n",
    "    conda_dependencies=conda_dep)\n",
    "run_config.target = compute_target.name\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.gpu_support = True\n",
    "run_config.environment.docker.base_image = 'tsmatz/azureml-openmpi:0.1.0-gpu'\n",
    "run_config.communicator = 'OpenMpi'\n",
    "run_config.node_count = 4\n",
    "run_config.mpi.process_count_per_node = 1\n",
    "\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='start_mx_role.py',\n",
    "    run_config=run_config,\n",
    "    arguments=[\n",
    "        '--num_workers', 2,\n",
    "        '--num_servers', 1,\n",
    "        '--scheduler_host', '`cut -d \":\" -f 1 <<< $AZ_BATCH_MASTER_NODE`']) # getting master node's ip like \"10.0.0.4\" (or use $AZ_BATCHAI_MPI_MASTER_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: mnist_mxnet_distributed_1547784855230\n",
      "\n",
      "Streaming azureml-logs/60_control_log_rank_0.txt\n",
      "================================================\n",
      "\n",
      "This is an MPI job. Rank:0\n",
      "Streaming log file azureml-logs/60_control_log_rank_0.txt\n",
      "Streaming log file azureml-logs/80_driver_log_rank_0.txt\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_2.txt\n",
      "===============================================\n",
      "\n",
      "Downloading /root/.mxnet/datasets/mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading /root/.mxnet/datasets/mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading /root/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading /root/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n",
      "Epoch 0: Accuracy 0.927800\n",
      "Epoch 1: Accuracy 0.944000\n",
      "Epoch 2: Accuracy 0.959600\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_0.txt\n",
      "===============================================\n",
      "\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.10052657127380371 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: mnist_mxnet_distributed_1547784855230\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'mnist_mxnet_distributed_1547784855230',\n",
       " 'target': 'cluster01',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2019-01-18T04:14:32.472192Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': 'a56c6fac-3a7e-4788-840a-fd38eef9810b'},\n",
       " 'runDefinition': {'Script': 'start_mx_role.py',\n",
       "  'Arguments': ['--num_workers',\n",
       "   '2',\n",
       "   '--num_servers',\n",
       "   '1',\n",
       "   '--scheduler_host',\n",
       "   '`cut -d \":\" -f 1 <<< $AZ_BATCH_MASTER_NODE`'],\n",
       "  'SourceDirectoryDataStore': None,\n",
       "  'Framework': 0,\n",
       "  'Communicator': 5,\n",
       "  'Target': 'cluster01',\n",
       "  'DataReferences': {},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'NodeCount': 4,\n",
       "  'Environment': {'Python': {'InterpreterPath': 'python',\n",
       "    'UserManagedDependencies': False,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults==1.0.6', 'mxnet-cu90', 'mpi4py']}]}},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'Docker': {'BaseImage': 'tsmatz/azureml-openmpi:0.1.0-gpu',\n",
       "    'Enabled': True,\n",
       "    'SharedVolumes': True,\n",
       "    'Preparation': None,\n",
       "    'GpuSupport': True,\n",
       "    'ShmSize': '1g',\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 0},\n",
       "  'AmlCompute': {'Name': None,\n",
       "   'VmSize': None,\n",
       "   'VmPriority': None,\n",
       "   'RetainCluster': False,\n",
       "   'ClusterMaxNodeCount': 1},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log_rank_0.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/60_control_log_rank_0.txt?sv=2018-03-28&sr=b&sig=61Y8mV1hzWJkA7BajfOfMfjq6vkAgIKG3QaoaUdnS8Y%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/60_control_log_rank_1.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/60_control_log_rank_1.txt?sv=2018-03-28&sr=b&sig=QmupfywtiQBemqr9fbELtvt%2BHduapc9%2Bii7eF7K6pX4%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/60_control_log_rank_3.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/60_control_log_rank_3.txt?sv=2018-03-28&sr=b&sig=AigYcnSzqMYLhXLGicK50FC3KFjyStnRjan6nKn6%2BYM%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/60_control_log_rank_2.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/60_control_log_rank_2.txt?sv=2018-03-28&sr=b&sig=2QYvJXQpFwTelsPW%2BuXYdjabS0JEuBq%2B6CdT%2FeclqLY%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_3.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/80_driver_log_rank_3.txt?sv=2018-03-28&sr=b&sig=WwBXt6KZX19fiKF5uvvfRU5%2BOb%2FT9PE6unZkMeB%2Bgak%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_2.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/80_driver_log_rank_2.txt?sv=2018-03-28&sr=b&sig=M3fGZqt7npK3eqAcy9Gh%2BaqstbRQxVaoP3TOX%2FLGkNY%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=vxAtBAdxl3I90rgJuzzKUW9Zv1IRhr4kGPVLli9sB6Q%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_1.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/80_driver_log_rank_1.txt?sv=2018-03-28&sr=b&sig=bCQp4mk8CLr3yDnhcUJPBsl055UAaJTAIEGurOH4oXc%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r',\n",
       "  'azureml-logs/80_driver_log_rank_0.txt': 'https://ws015719760712.blob.core.windows.net/azureml/ExperimentRun/dcid.mnist_mxnet_distributed_1547784855230/azureml-logs/80_driver_log_rank_0.txt?sv=2018-03-28&sr=b&sig=d0CuOL0LIypnnjG6e1H8hdqzXzLbP1wqI1vnlhY7ddQ%3D&st=2019-01-18T04%3A08%3A08Z&se=2019-01-18T12%3A18%3A08Z&sp=r'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(workspace=ws, name='mnist_mxnet_distributed')\n",
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. See Generated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/60_control_log_rank_0.txt',\n",
       " 'azureml-logs/60_control_log_rank_1.txt',\n",
       " 'azureml-logs/60_control_log_rank_3.txt',\n",
       " 'azureml-logs/60_control_log_rank_2.txt',\n",
       " 'azureml-logs/80_driver_log_rank_3.txt',\n",
       " 'azureml-logs/80_driver_log_rank_2.txt',\n",
       " 'outputs/test-0001.params',\n",
       " 'outputs/test-symbol.json',\n",
       " 'driver_log',\n",
       " 'azureml-logs/azureml.log',\n",
       " 'azureml-logs/80_driver_log_rank_1.txt',\n",
       " 'azureml-logs/80_driver_log_rank_0.txt',\n",
       " 'azureml-logs/56_batchai_stderr.txt',\n",
       " 'azureml-logs/55_batchai_execution-tvm-676767296_4-20190118t040928z.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see and download results (test-symbol.json, test-0001.params).\n",
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Remove Cluster (Clean-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (nodes) and remove from AML workspace\n",
    "mycompute = AmlCompute(workspace=ws, name='cluster01')\n",
    "mycompute.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
